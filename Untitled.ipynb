{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0e09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import utils\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83953fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus=torch.cuda.device_count()\n",
    "rank=0\n",
    "os.environ['MASTER_ADDR']='localhost'\n",
    "os.environ['MASTER_PORT']='7777'\n",
    "\n",
    "hps=utils.get_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88f68b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/caijb/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/home/caijb/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/caijb/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "  File \"/home/caijb/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_and_eval' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'train_and_eval' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5812/1336908710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    228\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[1;32m    229\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 )\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    140\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "mp.spawn(train_and_eval,nprocs=n_gpus,args=(n_gpus,hps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86318f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(rank, n_gpus, hps):\n",
    "  global global_step\n",
    "  if rank == 0:\n",
    "    logger = utils.get_logger(hps.model_dir)\n",
    "    logger.info(hps)\n",
    "    utils.check_git_hash(hps.model_dir)\n",
    "    writer = SummaryWriter(log_dir=hps.model_dir)\n",
    "    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n",
    "\n",
    "\n",
    "  print(\"iniialize process group\")\n",
    "\n",
    "  dist.init_process_group(backend='gloo', init_method=\"env://\", world_size=n_gpus, rank=rank)\n",
    "  #dist.init_process_group(backend='nccl',world_size=n_gpus, rank=rank)\n",
    "\n",
    "  print(\"finished\")\n",
    "  torch.manual_seed(hps.train.seed)\n",
    "  torch.cuda.set_device(rank)\n",
    "\n",
    "  print(\"Loading Datasets\")\n",
    "\n",
    "  train_dataset = TextMelLoader(hps.data.training_files, hps.data)\n",
    "  #train_dataset = TextMelSpeakerLoader(hps.data.training_files, hps.data)\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=n_gpus,\n",
    "      rank=rank,\n",
    "      shuffle=True)\n",
    "  collate_fn = TextMelCollate(1)\n",
    "  train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False,\n",
    "      batch_size=hps.train.batch_size, pin_memory=True,\n",
    "      drop_last=True, collate_fn=collate_fn, sampler=train_sampler)\n",
    "  if rank == 0:\n",
    "    val_dataset = TextMelLoader(hps.data.validation_files, hps.data)\n",
    "    val_loader = DataLoader(val_dataset, num_workers=8, shuffle=False,\n",
    "        batch_size=hps.train.batch_size, pin_memory=True,\n",
    "        drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "  print(\"Loading Models\")\n",
    "\n",
    "  generator = models.FlowGenerator(\n",
    "      n_vocab=len(symbols) + getattr(hps.data, \"add_blank\", False), \n",
    "      out_channels=hps.data.n_mel_channels, \n",
    "      **hps.model).cuda(rank)\n",
    "  optimizer_g = commons.Adam(generator.parameters(), scheduler=hps.train.scheduler, dim_model=hps.model.hidden_channels, warmup_steps=hps.train.warmup_steps, lr=hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "  \n",
    "  if hps.train.fp16_run:\n",
    "    generator, optimizer_g._optim = amp.initialize(generator, optimizer_g._optim, opt_level=\"O1\")\n",
    "  generator = DDP(generator)\n",
    "  \n",
    "  epoch_str = 1\n",
    "  global_step = 0\n",
    "  try:\n",
    "    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), generator, optimizer_g)\n",
    "    epoch_str += 1\n",
    "    optimizer_g.step_num = (epoch_str - 1) * len(train_loader)\n",
    "    optimizer_g._update_learning_rate()\n",
    "    global_step = (epoch_str - 1) * len(train_loader)\n",
    "  except:\n",
    "    if hps.train.ddi and os.path.isfile(os.path.join(hps.model_dir, \"ddi_G.pth\")):\n",
    "      _ = utils.load_checkpoint(os.path.join(hps.model_dir, \"ddi_G.pth\"), generator, optimizer_g)\n",
    "  \n",
    "  print(\"Training will start soon\")\n",
    "  for epoch in range(epoch_str, hps.train.epochs + 1):\n",
    "    if rank==0:\n",
    "      train(rank, epoch, hps, generator, optimizer_g, train_loader, logger, writer)\n",
    "      evaluate(rank, epoch, hps, generator, optimizer_g, val_loader, logger, writer_eval)\n",
    "      utils.save_checkpoint(generator, optimizer_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(epoch)))\n",
    "    else:\n",
    "      train(rank, epoch, hps, generator, optimizer_g, train_loader, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a290f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
