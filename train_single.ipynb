{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8503d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "from apex import amp\n",
    "\n",
    "from data_utils import TextMelLoader, TextMelCollate , TextMelSpeakerLoader, TextMelSpeakerCollate\n",
    "import models\n",
    "import commons\n",
    "import utils\n",
    "from text.symbols import symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ca7bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74ba4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n",
    "  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n",
    "\n",
    "  n_gpus = torch.cuda.device_count()\n",
    "  rank=1\n",
    "  os.environ['MASTER_ADDR'] = 'localhost'\n",
    "  os.environ['MASTER_PORT'] = '9000'\n",
    "\n",
    "  hps = utils.get_hparams()\n",
    "  #mp.spawn(train_and_eval, nprocs=n_gpus, args=(n_gpus, hps))\n",
    "  train_and_eval(rank,n_gpus,hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c46436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(rank, n_gpus, hps):\n",
    "  global global_step\n",
    "  if rank == 0:\n",
    "    logger = utils.get_logger(hps.model_dir)\n",
    "    logger.info(hps)\n",
    "    utils.check_git_hash(hps.model_dir)\n",
    "    writer = SummaryWriter(log_dir=hps.model_dir)\n",
    "    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n",
    "\n",
    "  dist.init_process_group(backend='nccl', init_method=\"env://\", world_size=n_gpus, rank=rank)\n",
    "  #dist.init_process_group(backend='nccl',world_size=n_gpus, rank=rank)\n",
    "  torch.manual_seed(hps.train.seed)\n",
    "  torch.cuda.set_device(rank)\n",
    "\n",
    "  train_dataset = TextMelLoader(hps.data.training_files, hps.data)\n",
    "  #train_dataset = TextMelSpeakerLoader(hps.data.training_files, hps.data)\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=n_gpus,\n",
    "      rank=rank,\n",
    "      shuffle=True)\n",
    "  collate_fn = TextMelCollate(1)\n",
    "  train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False,\n",
    "      batch_size=hps.train.batch_size, pin_memory=True,\n",
    "      drop_last=True, collate_fn=collate_fn, sampler=train_sampler)\n",
    "  if rank == 0:\n",
    "    val_dataset = TextMelLoader(hps.data.validation_files, hps.data)\n",
    "    val_loader = DataLoader(val_dataset, num_workers=8, shuffle=False,\n",
    "        batch_size=hps.train.batch_size, pin_memory=True,\n",
    "        drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "  generator = models.FlowGenerator(\n",
    "      n_vocab=len(symbols) + getattr(hps.data, \"add_blank\", False), \n",
    "      out_channels=hps.data.n_mel_channels, \n",
    "      **hps.model).cuda(rank)\n",
    "  optimizer_g = commons.Adam(generator.parameters(), scheduler=hps.train.scheduler, dim_model=hps.model.hidden_channels, warmup_steps=hps.train.warmup_steps, lr=hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "  if hps.train.fp16_run:\n",
    "    generator, optimizer_g._optim = amp.initialize(generator, optimizer_g._optim, opt_level=\"O1\")\n",
    "  generator = DDP(generator)\n",
    "  epoch_str = 1\n",
    "  global_step = 0\n",
    "  try:\n",
    "    _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"), generator, optimizer_g)\n",
    "    epoch_str += 1\n",
    "    optimizer_g.step_num = (epoch_str - 1) * len(train_loader)\n",
    "    optimizer_g._update_learning_rate()\n",
    "    global_step = (epoch_str - 1) * len(train_loader)\n",
    "  except:\n",
    "    if hps.train.ddi and os.path.isfile(os.path.join(hps.model_dir, \"ddi_G.pth\")):\n",
    "      _ = utils.load_checkpoint(os.path.join(hps.model_dir, \"ddi_G.pth\"), generator, optimizer_g)\n",
    "  \n",
    "  for epoch in range(epoch_str, hps.train.epochs + 1):\n",
    "    if rank==0:\n",
    "      train(rank, epoch, hps, generator, optimizer_g, train_loader, logger, writer)\n",
    "      evaluate(rank, epoch, hps, generator, optimizer_g, val_loader, logger, writer_eval)\n",
    "      utils.save_checkpoint(generator, optimizer_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(epoch)))\n",
    "    else:\n",
    "      train(rank, epoch, hps, generator, optimizer_g, train_loader, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace9c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, epoch, hps, generator, optimizer_g, train_loader, logger, writer):\n",
    "  train_loader.sampler.set_epoch(epoch)\n",
    "  global global_step\n",
    "\n",
    "  generator.train()\n",
    "  for batch_idx, (x, x_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "    x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n",
    "    y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n",
    "\n",
    "    # Train Generator\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    (z, z_m, z_logs, logdet, z_mask), (x_m, x_logs, x_mask), (attn, logw, logw_) = generator(x, x_lengths, y, y_lengths, gen=False)\n",
    "    l_mle = commons.mle_loss(z, z_m, z_logs, logdet, z_mask)\n",
    "    l_length = commons.duration_loss(logw, logw_, x_lengths)\n",
    "\n",
    "    loss_gs = [l_mle, l_length]\n",
    "    loss_g = sum(loss_gs)\n",
    "\n",
    "    if hps.train.fp16_run:\n",
    "      with amp.scale_loss(loss_g, optimizer_g._optim) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "      grad_norm = commons.clip_grad_value_(amp.master_params(optimizer_g._optim), 5)\n",
    "    else:\n",
    "      loss_g.backward()\n",
    "      grad_norm = commons.clip_grad_value_(generator.parameters(), 5)\n",
    "    optimizer_g.step()\n",
    "    \n",
    "    if rank==0:\n",
    "      if batch_idx % hps.train.log_interval == 0:\n",
    "        (y_gen, *_), *_ = generator.module(x[:1], x_lengths[:1], gen=True)\n",
    "        logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader),\n",
    "          loss_g.item()))\n",
    "        logger.info([x.item() for x in loss_gs] + [global_step, optimizer_g.get_lr()])\n",
    "        \n",
    "        scalar_dict = {\"loss/g/total\": loss_g, \"learning_rate\": optimizer_g.get_lr(), \"grad_norm\": grad_norm}\n",
    "        scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(loss_gs)})\n",
    "        utils.summarize(\n",
    "          writer=writer,\n",
    "          global_step=global_step, \n",
    "          images={\"y_org\": utils.plot_spectrogram_to_numpy(y[0].data.cpu().numpy()), \n",
    "            \"y_gen\": utils.plot_spectrogram_to_numpy(y_gen[0].data.cpu().numpy()), \n",
    "            \"attn\": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy()),\n",
    "            },\n",
    "          scalars=scalar_dict)\n",
    "    global_step += 1\n",
    "  \n",
    "  if rank == 0:\n",
    "    logger.info('====> Epoch: {}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d836fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rank, epoch, hps, generator, optimizer_g, val_loader, logger, writer_eval):\n",
    "  if rank == 0:\n",
    "    global global_step\n",
    "    generator.eval()\n",
    "    losses_tot = []\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (x, x_lengths, y, y_lengths) in enumerate(val_loader):\n",
    "        x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n",
    "        y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n",
    "\n",
    "        \n",
    "        (z, z_m, z_logs, logdet, z_mask), (x_m, x_logs, x_mask), (attn, logw, logw_) = generator(x, x_lengths, y, y_lengths, gen=False)\n",
    "        l_mle = commons.mle_loss(z, z_m, z_logs, logdet, z_mask)\n",
    "        l_length = commons.duration_loss(logw, logw_, x_lengths)\n",
    "\n",
    "        loss_gs = [l_mle, l_length]\n",
    "        loss_g = sum(loss_gs)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "          losses_tot = loss_gs\n",
    "        else:\n",
    "          losses_tot = [x + y for (x, y) in zip(losses_tot, loss_gs)]\n",
    "\n",
    "        if batch_idx % hps.train.log_interval == 0:\n",
    "          logger.info('Eval Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(x), len(val_loader.dataset),\n",
    "            100. * batch_idx / len(val_loader),\n",
    "            loss_g.item()))\n",
    "          logger.info([x.item() for x in loss_gs])\n",
    "           \n",
    "    \n",
    "    losses_tot = [x/len(val_loader) for x in losses_tot]\n",
    "    loss_tot = sum(losses_tot)\n",
    "    scalar_dict = {\"loss/g/total\": loss_tot}\n",
    "    scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_tot)})\n",
    "    utils.summarize(\n",
    "      writer=writer_eval,\n",
    "      global_step=global_step, \n",
    "      scalars=scalar_dict)\n",
    "    logger.info('====> Epoch: {}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf9ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9ddec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
